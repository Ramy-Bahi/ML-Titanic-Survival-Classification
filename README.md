# ğŸ›³ï¸ Titanic Survival Prediction â€” Binary Classification

This project explores **Machine Learning classification techniques** on the [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic) dataset from Kaggle.  
Itâ€™s part of my journey to fully master **Machine Learning** before moving into **Deep Learning**.

---

## ğŸ“˜ Project Overview

The goal is to predict **whether a passenger survived or not** during the Titanic disaster based on demographic, social, and ticket-related information.

This project is built in a single, complete notebook:

| Notebook | Description |
|-----------|--------------|
| ğŸ““ `titanic_binary_classification.ipynb` | Full end-to-end pipeline: data cleaning, feature engineering, encoding, balancing, model training, and evaluation. |

---

## âš™ï¸ Technologies Used

- **Python 3.x**
- **NumPy**, **Pandas** â†’ data manipulation  
- **Matplotlib**, **Seaborn** â†’ visualization  
- **Scikit-Learn** â†’ classification, preprocessing, evaluation  
- **Imbalanced-learn (SMOTE)** â†’ class balancing  

---

## ğŸ§© Data Preprocessing Pipeline

1. **Handle missing values** in `Age`, `Fare`, `Embarked`, etc.  
2. **Feature Engineering**:  
   - Extracted `Title` from passenger names  
   - Grouped rare titles  
   - Encoded `Sex` (male â†’ 0, female â†’ 1)  
3. **Encoding**:  
   - One-Hot Encoding for categorical features (`Embarked`, `Title`)  
4. **Balancing**:  
   - Applied **SMOTE** to balance the â€œSurvivedâ€ and â€œDiedâ€ classes  
5. **Scaling**:  
   - Standardized continuous features (`Age`, `Fare`) using `StandardScaler`  
6. **Model**:  
   - Trained a **Logistic Regression** classifier  
7. **Evaluation**:  
   - Accuracy, F1-score, Cross-Validation, Confusion Matrix, Classification Report, ROC Curve  

---

## ğŸ“Š Model Results

| Metric | Score |
|--------|--------|
| **Accuracy** | 0.8101 |
| **F1 Score** | 0.7875 |
| **Cross-Validation Mean** | 0.8255 |
| **ROC-AUC** | 0.87 |

### ğŸ§  Confusion Matrix

[[82 23]
[11 63]]

| Class | Precision | Recall | F1-Score | Support |
|--------|------------|--------|-----------|----------|
| **0.0 (Died)** | 0.88 | 0.78 | 0.83 | 105 |
| **1.0 (Survived)** | 0.73 | 0.85 | 0.79 | 74 |
| **Accuracy** | - | - | **0.81** | 179 |

---

## ğŸš€ Next Steps

- Experiment with **Decision Tree**, **Random Forest**, and **SVM** models  
- Perform **hyperparameter tuning** using `GridSearchCV`  
- Deploy using **Streamlit** for an interactive web interface  

---

## ğŸ§¾ Repository Structure

ML-Titanic-Survival-Classification/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ titanic_classification.ipynb
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

---

## ğŸ§© How to Run

```bash
git clone https://github.com/Ramy-Bahi/Titanic-Survival-Classification.git
cd Titanic-Survival-Classification
pip install -r requirements.txt
jupyter notebook notebooks/titanic_binary_classification.ipynb
```

---

## ğŸ“Š Dataset

Available on Kaggle: [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)

---

## ğŸ‘¨â€ğŸ’» Author

**Rami Bahi**

ğŸ“ Masterâ€™s Student in Artificial Intelligence

ğŸ’» Passionate about Machine Learning, Deep Learning & Web Development

---

## â­ If you like this project...

Give it a star â­ on GitHub to support my work and journey!
